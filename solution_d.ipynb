{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9998d6",
   "metadata": {},
   "source": [
    "# Task 1: Download and transcribe the given the audio file using Speech-to-Text recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "# Load the pre-trained Wav2Vec model and tokenizer\n",
    "model = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-large-960h')\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained('facebook/wav2vec2-large-960h')\n",
    "\n",
    "# Load the audio file\n",
    "audio, _ = torchaudio.load('sales_call_telephone_marketers.wav')\n",
    "\n",
    "# Convert the audio to the required format\n",
    "input_values = tokenizer(audio.numpy(), return_tensors='pt').input_values\n",
    "\n",
    "# Transcribe the audio file\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d172101",
   "metadata": {},
   "source": [
    "# Task 2: Train an NLU model to classify intents and recognize entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df306b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Load the pre-trained spaCy English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a DocBin to store the labeled data\n",
    "doc_bin = DocBin()\n",
    "\n",
    "# Add the labeled example sentences to the DocBin\n",
    "doc = nlp('My name is Jeff and I am calling from Amazon.')\n",
    "doc.ents = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "doc_bin.add(doc)\n",
    "doc = nlp('I am calling from Microsoft and my name is Satya.')\n",
    "doc.ents = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "doc_bin.add(doc)\n",
    "doc = nlp('I am Sundar and this is call from Google.')\n",
    "doc.ents = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "doc_bin.add(doc)\n",
    "doc = nlp('I am calling about your Microsoft Azure subscription.')\n",
    "doc.ents = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "doc_bin.add(doc)\n",
    "doc = nlp('This is a call regarding your Google Cloud Platform account.')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311df9c",
   "metadata": {},
   "source": [
    "# Task 3: Separate the sentences in the output of task 1. On each sentence, apply the model trained in task 2 to classify its intent and recognize the entities present in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d2b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 3\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import json\n",
    "\n",
    "# Load the pre-trained Wav2Vec model and tokenizer\n",
    "model = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-large-960h')\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained('facebook/wav2vec2-large-960h')\n",
    "\n",
    "# Load the pre-trained spaCy English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load the NLU model trained in Q2\n",
    "nlp2 = spacy.load('nlu_model')\n",
    "\n",
    "# Load the audio file\n",
    "audio, _ = torchaudio.load('audio_file.wav')\n",
    "\n",
    "# Convert the audio to the required format\n",
    "input_values = tokenizer(audio.numpy(), return_tensors='pt').input_values\n",
    "\n",
    "# Transcribe the audio file and split the sentences\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "sentences = transcription.split('. ')\n",
    "\n",
    "# Classify the intent and recognize the entities for each sentence\n",
    "output = []\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    intent = nlp2(sentence).cats\n",
    "    entities = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "    output.append({'sentence': sentence, 'intent': intent, 'entities': entities})\n",
    "\n",
    "# Export the output in a JSON file\n",
    "with open('output.json', 'w') as f:\n",
    "    json.dump(output, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f88ad",
   "metadata": {},
   "source": [
    "# Task 4: For the whole text generated from the audio file generate a summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8013189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Clean the text\n",
    "text = text.lower() # convert to lowercase\n",
    "text = text.replace('\\n', ' ') # remove newlines\n",
    "text = text.replace('\\r', ' ') # remove carriage returns\n",
    "\n",
    "# Tokenize the text\n",
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Calculate sentence scores\n",
    "sentence_scores = []\n",
    "for sentence in sentences:\n",
    "    score = sum(word_counts[word] for word in word_tokenize(sentence))\n",
    "    sentence_scores.append((sentence, score))\n",
    "\n",
    "# Select top sentences\n",
    "n = 3 # desired length of summary\n",
    "top_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "# Concatenate selected sentences\n",
    "summary = ' '.join(sentence[0] for sentence in top_sentences)\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
